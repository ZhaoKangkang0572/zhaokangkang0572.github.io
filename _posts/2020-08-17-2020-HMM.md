---
layout: post
title: HMM
category: DLTheory
tags: HMM
description: 为了全面理解HMM,比如HMM为什么是这样的，为什么要对联合概率建模，
---

* content
{:toc}
#### 先要全面了解频率学派和贝叶斯学派

#### 马尔可夫模型是一种序列标注算法，从序列标注的角度理解
网上的很多博客，李老师的统计学都举的是盒子里取球的例子，我理解了HMM算法的计算推导，但是还是有很多“为什么”，这里用一个序列标注做例子，借此梳理一下
> 例
> 给（马尔可夫）标注（a,b,c,d）
>
> 首先我们描述问题：给定观测序列（句子序列）$(o_1,o_2,o_3,o_4)$,求令条件概率$P(o_1,o_2,o_3,o_4|s_1,s_2,s_3,s_4)$达到最大值的那个标注序列（隐藏状态）
> 换句话说，在什么样的标注序列的条件上，给定的句子（观测序列）的概率是最大的
> 什么样的$(s_1,s_2,s_3,s_4)$使得$(o_1,o_2,o_3,o_4)$概率最大
>> 这里出现了第一个为什么，我们明明是已知句子序列，不应该是求什么样的标注序列概率最大吗？也就是不能直接求$P(I|O)$吗？
>> 假如说，我们直接求$max P(I|O)$,那么我们要计算的就是$P(I_1|O_1)P(I_2|O_2)P(I_3|O_3)P(I_4|O_4)$ $P(I_i|O_i)$就是某个汉字所对应最可能的标注，但是这样的话，对于同一个汉字，统计出来最可能的标注是唯一的，显然这么做是不对的
>> 进一步，我们加入状态转换，计算 $P(I_1|O_1)P(I_2|O_2)P(I_2|I_1)P(I_3|O_3)P(I_3|I_2)P(I_4|O_4)P(I_4|I_3)$ 加入一个状态转换$P(I_i|I_{i_1})$
>> 举例就是计算$P(a|马)P(b|尔)P(b|a)P(c|可)P(c|b)P(d|夫)P(d|c)$ 这就回到了HMM，HMM做的就是这个，为了计算这个，HMM要统计**表现矩阵和状态转移矩阵**，接下来跳过下面的猜测直接看看HMM的计算过程
>> 我的猜测是：
>> 从全概率公式和贝叶斯公式的疑问
>> $P(A,B)=P(A|B)*P(B)$很好理解，AB同时发生的概率等于B发生的概率乘上B发生时A也发生的概率
>> 我们要求的$P(O|I)$，根据贝叶斯公式$P(O|I)=\frac{P(I|O)*(P(I))}{P(O)}$
>> 如果根据全概率公式：$P(O|I)=\frac{P(I,O)}{P(O)}$不是也可以吗
>> 这里就回到了频率学派的观点，认为$\theta$是唯一的，只是我们不知道，如果我们直接根据极大似然估计计算出$P(O|I)$，将$I$视为参数$\theta$，那么我们需要非常多非常多的训练集（也就是世界上所有科能出现的句子以及他们的标注），类比抛硬币，我们需要非常多的抛硬币试验做为训练集，才能算出概率为0.5，回到序列标注，我们几乎不可能得到那么多的数据集，所以接下来
>> 根据贝叶斯学派，我们认定了参数$\theta$服从一定的分布，也就是$p(\theta)$,根据$P(O|\theta)=\frac{P(\theta|O)*(P(\theta))}{P(O)}$，结合已知的观测信息($p(O)$)求出最大后验概率
>> 注意这里的$P(\theta|O)$不需要完全的先验信息，为什么() 这就是为什么HMM不直接用极大似然估计的原因
>> 根据上面的结论，我们容易知道，在什么时候求贝叶斯学派求最大后验和频率学派直接求最大似然估计的结果一样
>> 答案就是，随着样本增加，先验信息$p(\theta)$越来越不重要，直到样本信息完全，这两者的结果就是一样的了

>>所以，学习HMM，我们要特别关注$P(\theta|O)$和隐藏状态的分布$P(\theta)$是如何产生和计算的

>总的来说，不直接使用最大似然估计计算$P(O|I)$的原因有很多，下面两条我认为是主要原因
>极大似然的前提条件有两个
> 1. 极大似然估计要求样本独立且同分布（IID条件）， 要不然无法用概率密度函数乘积（抛硬币，三次正面的概率$p*p*p$）
> 2.样本的分布要符合真实分布，越接近真实分布，估计越准确。
#### HMM计算过程

序列标注的顺序就是利用训练数据和观测序列 先学习参数，也就是**解决第二个问题，学习问题**，然后解码求出最可能的状态序列，也就是**解码问题**
#### 概率计算
给定$\lambda=[\pi,A,B]$(初始状态概率分布，观测概率矩阵，状态转移概率矩阵)，求一个给定的观测序列X的概率，也就是$P(X|\lambda)$
这个很好算，前向后向都可以

#### 学习问题
给定观测序列，求$\lambda=[\pi,A,B]$
这里涉及到EM算法(Expectation Maximization)
主要思想就是猜想，优化两个步骤
我们现在只知道观测序列，所以猜测$\lambda$，然后根据观测序列来优化$\lambda$ 
>（李航老师的书上写的是(E步：求期望；M步，求极大)
>换句话说博客上看到的，这个过程可以简化为 猜（E-step）,反思（M-step）,重复；）
>简单介绍 https://www.jianshu.com/p/1121509ac1dc


#### 解码问题

给定观测序列，和$\lambda=[\pi,A,B]$，求使得给定观测序列的条件概率$P(I|O)$最大的状态序列$I=(I_1,I...I_i)$


#### EM算法
给定观测值x,关于$\theta$的似然函数$L(\theta|x)$（在数值上）等于给定参数$\theta$后变量x的概率:$L(\theta|x)=P(X=x|\theta)$
对于序列标注来说
给定观测$O=(O_1,O_2,...,O_N)$，隐含状态为$I=(I_1,I_2,...,I_N)$, 模型参数$\theta$
这时我们有$p(x_i)=p(x_i|\theta)$,$p(x_i,I_i)=p(x_i,I_i|\theta)$

>为什么我们不用极大似然函数直接求出$\theta$？
>假如没有隐含状态$I_i$，我们可以构造对数似然函数来求极大似然函数
>$$\theta=\mathop{\arg\max}_\theta L(\theta|x)=>\mathop{\arg\max}_\theta \ln \prod_{i=1}^N P(x_i|\theta)=\mathop{\arg\max}_\theta \sum_{i=1}^N \ln(p(x_1|\theta))$$
>但是，增加隐藏状态之后，根据边缘概率合成联合概率公式，我们有
$$\theta,I=\mathop{\arg\max}_{\theta,z} L(\theta,I|x)=>=\mathop{\arg\max}_\theta \sum_{i=1}^N \ln \sum_{I_i}(p(x_i,I_i|\theta))$$
>我们可以对$\theta,z$求偏导来使似然函数极大化
>我们知道$P(x_i|\theta)$是$P(x_i,I_i)|\theta$的边缘概率，要求导非常复杂，因为
>$$\ln \sum_{I_i}(p(x_i,I_i|\theta):需要把I的所有可能连加$$
>先将加号从log里面提取出来,我们构造一个函数$Q(I_i)$对下列式子缩放
>满足: $\sum \limits_{I} Q_i(I)=1,0<=Q_i(I)<=1$
>$$\sum_{i=1}^N \ln \sum_{I_i}(p(x_i,I_i|\theta)=\sum_{i=1}^N \ln \sum_{I_i}Q_i(I_i)\frac{(p(x_i,I_i|\theta)}{Q_i(I_i)}\\ >= 
\sum_{i=1}^N  \sum_{I_i}Q_i(I_i) \ln \frac{(p(x_i,I_i|\theta)}{Q_i(I_i)}$$
>
>这里用到了Jensen不等式 $ln(E(y))>=E(log(y))$
>其中：$$E(y)=\sum \limits_i \lambda_iy_i, \lambda_i>=0,\sum_i \lambda_i=1$$
>
>对应我们构造的式子
>$$y_i=\frac{p(x_i,I_i|\theta)}{Q_i(I_i)}$$
>$$\lambda_i=Q_i(I_i)$$
>也就是说$\frac{p(x_i,I_i|\theta)}{Q_i(I_i)}$是第i个样本， $Q_i(I_i)$是对应权重 那么
>$$E(\ln \frac{p(x_i,I_i|\theta)}{Q_i(I_i)})=\sum\limits_{I_i}Q_i(I_i) \ln \frac{p(x_i,I_i|\theta)}{Q_i(I_i)}$$
>上式我们构建了$L(\theta,I)$的下界，实际上这就是 $\ln \frac {p(x_i,I_i|\theta)}{Q_i(I_i)}$的加权求和，也就是期望，这里就是E步，expection的来历

>下一步就是寻找合适的$Q_i(I)$来最优化这个下界，也就是M步
>假设$\theta$给定，那么$\ln L(\theta)$的值就取决于$Q_i(I)$和$P(x_i,z_i)$，我们可以同通过调整这两个概率使得下界逼近$\ln L(\theta)$的真实值，当不等式变成等式的时候，说明我们调整后的下界能够等价于$\ln L(\theta)$，
>由jensen不等式可知，等式成立的条件是随机变量为常数，则有：
>$$\frac {p(x_i,I_i|\theta)}{Q_i(I_i)}=C$$
>对于任意$i$，我们得到：
>$$P(x_i,I_i|\theta)=CQ_i(I_i)$$
>方程两边同时累加
>$$\sum \limits_I P(x_i,I_i|\theta)=C\sum \limits_I  Q_i(I_i)$$
>由于$\sum \limits_I Q_i(I_i)=1$,根据
>边缘概率公式 $P(x_i|\theta)=\sum \limits_I P(x_i,I_i|\theta)$
>条件概率公式 $P(I_i|x_i,\theta)=\frac{P(x_i,I_i|\theta)}{P(x_i|\theta)}$
>可得
>$$\sum \limits_I P(x_i,I_i|\theta)=C\sum \limits_I  Q_i(I_i)=C$$
>$$Q_i(I_i)=\frac{P(x_i,I_i|\theta)}{C}=\frac{P(x_i,I_i|\theta}{\sum \limits_I P(x_i,I_i|\theta} \\ = \frac{P(x_i,I_i|\theta}{P(x_i|\theta)}=P(I_i|x_i,\theta)$$
>从上式可发现 $Q(I)$是已知样本和模型参数下的隐藏状态分布
>如果$Q_i(I_i)=P(I_i|x_i,\theta)$，则 $\sum_{i=1}^N  \sum_{I_i}Q_i(I_i) \ln \frac{(p(x_i,I_i|\theta)}{Q_i(I_i)}$是我们包含隐藏状态的对数似然的一个下界，如果能极大化这个下界，则也是在尝试极大化我们的对数似然
>即我们需要极大化下式：
>$$\mathop{\arg\max}_{\theta}
\sum_{i=1}^N  \sum_{I_i}Q_i(I_i) \ln \frac{p(x_i,I_i|\theta)}{Q_i(I_i)}$$

>至此，我们推出了在**固定参数$\theta$**后分布$Q_I(i_i)$的选择问题，从而建立的$\ln L(\theta)$的下界
>接下来的M步就是固定$Q_i(I_i)$后，调整$\theta$，来极大化$\ln L(\theta)$的下界，去掉上式中常数部分$Q_i(I_i)$，则我们需要极大化的对数似然下界为：
>$$\mathop{\arg\max}_{\theta}
\sum_{i=1}^N  \sum_{I_i}Q_i(I_i) \ln p(x_i,I_i|\theta)$$

#### 总结一下
首先，我们已知的 $X=(x_1,x_2,...,x_N)$ 和模型，但是模型的参数我们并不知道
我们要干什么？我们要利用观察样本反推参数
反推参数的第一考虑当然是最大似然估计 也就是求$\mathop{\arg\max}_{\theta} P(x|\theta)$
一般来说，我们可以通过求导直接求解，或者用梯度下降法去迭代
但是不幸的是，在一些情况下，我们的模型必须引入一个隐藏变量，否则 $P(x|\theta)$不能在这个模型上表现出来
所以我们把原来极大似然估计：
>$$\theta=\mathop{\arg\max}_\theta L(\theta|x)=>\mathop{\arg\max}_\theta \ln \prod_{i=1}^N P(x_i|\theta)=\mathop{\arg\max}_\theta \sum_{i=1}^N \ln(p(x_1|\theta))$$
>增加隐藏状态之后，根据边缘概率合成联合概率公式，改写成
$$\theta,I=\mathop{\arg\max}_{\theta,z} L(\theta,I|x)=>=\mathop{\arg\max}_\theta \sum_{i=1}^N \ln \sum_{I_i}(p(x_i,I_i|\theta))$$
>尽管我们可以对$\theta,z$求偏导来使似然函数极大化，但是由于$P(x_i|\theta)$是$P(x_i,I_i|\theta)$的边缘概率，要求导非常复杂，因为
>$$\ln \sum_{I_i}(p(x_i,I_i|\theta):需要把I的所有可能连加$$
>这样就自然而然地引入了EM算法：
>EM算法就是为了解决有隐藏状态的最大似然估计算法，EM算法就是构造了一个$P(x|\theta)$的下界，然后通过不断优化这个下界函数的最大值来逼近实际的最大值
1. 确定目标函数
 我们代入隐藏状态将下列式子改写
 $$\theta=\mathop{\arg\max}_{\theta}\ln L(\theta)=\mathop{\arg\max}_{\theta} \ln \prod_{i=1}^{N}P(x_i|\theta)=\mathop{\arg\max}_{\theta} \sum_{i=1}^{N}\ln P(x_i|\theta)  $$
 变成
 $$\theta,I=\mathop{\arg\max}_{\theta,z} L(\theta,I|x)=>=\mathop{\arg\max}_\theta \sum_{i=1}^N \ln \sum_{I_i}(p(x_i,I_i|\theta))$$
2. 建立下界
 >为什么要建立下界
 >我们的目标是极大化我们的对数似然函数，所以极大化这个下界,则也是在极大化似然函数
 >当不等式变成等式的时候，说明我们最优化的下界能够等价于极大似然函数
利用jensen不等式建立下界,这样做还有另一个好处，就是可以把连加提取出来
$$E(f(x))\ge f(E(x))$$
满足: $\sum \limits_{I} Q_i(I)=1,0<=Q_i(I)<=1$，这样做就给$p(x_i,I_i|\theta)$人为构建了一个求期望的式子，加上前面的连加符号正好满足jensen不等式： $\sum_{I_i}Q_i(I_i)\frac{(p(x_i,I_i|\theta)}{Q_i(I_i)}$
$$\sum_{i=1}^N \ln \sum_{I_i}p(x_i,I_i|\theta)=\sum_{i=1}^N \ln \sum_{I_i}Q_i(I_i)\frac{(p(x_i,I_i|\theta)}{Q_i(I_i)}\\ >= 
\sum_{i=1}^N  \sum_{I_i}Q_i(I_i) \ln \frac{p(x_i,I_i|\theta)}{Q_i(I_i)}$$

>推导为什么优化下界就可以
 > 目标是大于等于变成等于，根据jensen不等式，变成等于号的条件是随机变量为常数
 > 也就是$\frac{p(x_i,I_i|\theta)}{Q_i(I_i)}=C$
 对于任意$i$，我们有：
$$P(x_i,I_i|\theta)=CQ_i(I_i)$$
方程两边同时累加，由于$\sum \limits_I Q_i(I_i)=1$,可得
$$\sum \limits_I P(x_i,I_i|\theta)=C\sum \limits_I  Q_i(I_i)=C$$
再次优化
>$$Q_i(I_i)=\frac{P(x_i,I_i|\theta)}{C}=\frac{P(x_i,I_i|\theta}{\sum \limits_I P(x_i,I_i|\theta} \\ = \frac{P(x_i,I_i|\theta}{P(x_i|\theta)}=P(I_i|x_i,\theta)$$
>如果$Q_i(I_i)=P(I_i|x_i,\theta)$，则 $\sum_{i=1}^N  \sum_{I_i}Q_i(I_i) \ln \frac{(p(x_i,I_i|\theta)}{Q_i(I_i)}$是我们包含隐藏状态的对数似然的一个下界，如果能极大化这个下界，则也是在尝试极大化我们的对数似然
>即我们需要极大化下式：
>$$\mathop{\arg\max}_{\theta}
\sum_{i=1}^N  \sum_{I_i}Q_i(I_i) \ln \frac{p(x_i,I_i|\theta)}{Q_i(I_i)}$$

3. 总结算法
以掷硬币为例

|轮数|硬币类型  | 结果  | 统计正| 统计反  |
|:---: |:---:  |:---:  |:---:    |:---:|
| 1 | UKN|11010  |  3 | 2   |
| 2 | UKN| 00110  | 2  |   3 |
| 3 | UKN| 10000  |  1 | 4   |
| 4 | UKN| 10011  | 3  | 2   |
| 4 | UKN| 01100  | 2  | 3   |

#### E步做了什么
 1. 初始化 每种硬币抛出正面的概率
 $\hat{\theta_a}=0.2,\hat{\theta_b}=0.7$
 >E 步的目标就是在这个初始化的参数条件下，计算每组观测数据出自A硬币和出自B硬币的概率
2. 计算这个概率
 如果第一轮数据的五次都是A硬币投的，
 那么得到11010的概率是$0.2^3*0.8^2=0.00512$
 如果第一轮数据的五次都是A硬币投的，
  那么得到11010的概率是$0.7^3*0.3^2=0.00512$
  那么第一组实验结果是A硬币得到的概率为：$\frac{0.00512 }{ 0.00512 + 0.03087}=0.14$，第一组实验结果是B硬币得到的概率为：$\frac{0.03087} {0.00512 + 0.03087}=0.86$
  我们给五组都计算一下
  |轮数|A硬币  |B硬币  |
|:---: |:---:  |:---:  |
| 1 | 0.14|0.86  | 
| 2 | 0.61| 0.39  | 
| 3 | 0.94| 0.06  | 
| 4 | 0.14| 0.86  |
| 4 | 0.61| 0.39  | 
3. 计算期望
 比如对于第一组数据
 如果是A硬币投掷的结果：0.14*3=0.42个正面和0.14*2=0.28个反面；如果是B硬币投掷的结果：0.86*3=2.58个正面和0.86*2=1.72个反面
 
 5组实验的期望如下表：
 |轮数|A硬币正面|A硬币反面  |B硬币正面  |B硬币反面|
|:---: |:---:  |:---:  |:---:  |:---:  |
| 1 | 0.42|0.28  | 2.58|1.72  | 
| 2 | 0.| 0.  | 0.|0.  | 
| 3 | 0.| 0.  | 0.|0.  | 
| 4 | 0.| 0.  |0.|0.  | 
| 4 | 0.| 0.  | 0.|0.  | 
| 总计 | 4.22| 7.98  | 6.78|6.02  | 
>通过计算期望，我们把一个有隐含变量的问题变化成了一个没有隐含变量的问题，然后就可以估计 $\hat{\theta_a},\hat{\theta_b}$

#### M步
 $\hat{\theta_a}=4.22/(4.22+7.98)=0.35$
 $\hat{\theta_b}=6.78/(6.78+6.02)=0.52968757$
 
 #### 迭代
 用新的参数去计算隐藏状态的期望
 用得到的期望计算新参数，直到参数不再更新为止
































Reference
https://www.zhihu.com/search?type=content&q=em%E7%AE%97%E6%B3%95
https://blog.csdn.net/bitcarmanlee/article/details/81417151