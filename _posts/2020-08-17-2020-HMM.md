---
layout: post
title: 2020-08-17-2020-HMM
category: DLTheory
tags: HMM
description: 为了全面理解HMM,比如HMM为什么是这样的，为什么要对联合概率建模，
---

* content
{:toc}
#### 先要全面了解频率学派和贝叶斯学派

#### 马尔可夫模型是一种序列标注算法，从序列标注的角度理解
网上的很多博客，李老师的统计学都举的是盒子里取球的例子，我理解了HMM算法的计算推导，但是还是有很多“为什么”，这里用一个序列标注做例子，借此梳理一下
> 例
> 给（马尔可夫）标注（a,b,c,d）
>
> 首先我们描述问题：给定观测序列（句子序列）$(o_1,o_2,o_3,o_4)$,求令条件概率$P(o_1,o_2,o_3,o_4|s_1,s_2,s_3,s_4)$达到最大值的那个标注序列（隐藏状态）
> 换句话说，在什么样的标注序列的条件上，给定的句子（观测序列）的概率是最大的
> 什么样的$(s_1,s_2,s_3,s_4)$使得$(o_1,o_2,o_3,o_4)$概率最大
>> 这里出现了第一个为什么，我们明明是已知句子序列，不应该是求什么样的标注序列概率最大吗？也就是不能直接求$P(I|O)$吗？
>> 假如说，我们直接求$max P(I|O)$,那么我们要计算的就是$P(I_1|O_1)P(I_2|O_2)P(I_3|O_3)P(I_4|O_4)$ $P(I_i|O_i)$就是某个汉字所对应最可能的标注，但是这样的话，对于同一个汉字，统计出来最可能的标注是唯一的，显然这么做是不对的
>> 进一步，我们加入状态转换，计算 $P(I_1|O_1)P(I_2|O_2)P(I_2|I_1)P(I_3|O_3)P(I_3|I_2)P(I_4|O_4)P(I_4|I_3)$ 加入一个状态转换$P(I_i|I_{i_1})$
>> 举例就是计算$P(a|马)P(b|尔)P(b|a)P(c|可)P(c|b)P(d|夫)P(d|c)$ 这就回到了HMM，HMM做的就是这个，为了计算这个，HMM要统计**表现矩阵和状态转移矩阵**，接下来跳过下面的猜测直接看看HMM的计算过程
>> 我的猜测是：
>> 从全概率公式和贝叶斯公式的疑问
>> $P(A,B)=P(A|B)*P(B)$很好理解，AB同时发生的概率等于B发生的概率乘上B发生时A也发生的概率
>> 我们要求的$P(O|I)$，根据贝叶斯公式$P(O|I)=\frac{P(I|O)*(P(I))}{P(O)}$
>> 如果根据全概率公式：$P(O|I)=\frac{P(I,O)}{P(O)}$不是也可以吗
>> 这里就回到了频率学派的观点，认为$\theta$是唯一的，只是我们不知道，如果我们直接根据极大似然估计计算出$P(O|I)$，将$I$视为参数$\theta$，那么我们需要非常多非常多的训练集（也就是世界上所有科能出现的句子以及他们的标注），类比抛硬币，我们需要非常多的抛硬币试验做为训练集，才能算出概率为0.5，回到序列标注，我们几乎不可能得到那么多的数据集，所以接下来
>> 根据贝叶斯学派，我们认定了参数$\theta$服从一定的分布，也就是$p(\theta)$,根据$P(O|\theta)=\frac{P(\theta|O)*(P(\theta))}{P(O)}$，结合已知的观测信息($p(O)$)求出最大后验概率
>> 注意这里的$P(\theta|O)$不需要完全的先验信息，为什么() 这就是为什么HMM不直接用极大似然估计的原因
>> 根据上面的结论，我们容易知道，在什么时候求贝叶斯学派求最大后验和频率学派直接求最大似然估计的结果一样
>> 答案就是，随着样本增加，先验信息$p(\theta)$越来越不重要，直到样本信息完全，这两者的结果就是一样的了

>>所以，学习HMM，我们要特别关注$P(\theta|O)$和隐藏状态的分布$P(\theta)$是如何产生和计算的

>总的来说，不直接使用最大似然估计计算$P(O|I)$的原因有很多，下面两条我认为是主要原因
>极大似然的前提条件有两个
> 1. 极大似然估计要求样本独立且同分布（IID条件）， 要不然无法用概率密度函数乘积（抛硬币，三次正面的概率$p*p*p$）
> 2.样本的分布要符合真实分布，越接近真实分布，估计越准确。
#### HMM计算过程

序列标注的顺序就是利用训练数据和观测序列 先学习参数，也就是**解决第二个问题，学习问题**，然后解码求出最可能的状态序列，也就是**解码问题**
#### 概率计算
给定$\lambda=[\pi,A,B]$(初始状态概率分布，观测概率矩阵，状态转移概率矩阵)，求一个给定的观测序列X的概率，也就是$P(X|\lambda)$
这个很好算，前向后向都可以

#### 学习问题
给定观测序列，求$\lambda=[\pi,A,B]$
这里涉及到EM算法(Expectation Maximization)
主要思想就是猜想，优化两个步骤
我们现在只知道观测序列，所以猜测$\lambda$，然后根据观测序列来优化$\lambda$ 
>（李航老师的书上写的是(E步：求期望；M步，求极大)
>换句话说博客上看到的，这个过程可以简化为 猜（E-step）,反思（M-step）,重复；）
>简单介绍 https://www.jianshu.com/p/1121509ac1dc


#### 解码问题

给定观测序列，和$\lambda=[\pi,A,B]$，求使得给定观测序列的条件概率$P(I|O)$最大的状态序列$I=(I_1,I...I_i)$

Reference
https://blog.csdn.net/bitcarmanlee/article/details/81417151