---
layout: post
title: BERT
category: MLTheory
tags: 
description: 
---
* content
{:toc}

#### 位置编码

位置编码有两个方向：
1. 单词的绝对位置
2. 单词的相对位置


>Transformer和BERT位置编码的区别
>都是相加
>Transformer:sinusoidal position encoding， 通过正余弦函数生成
>BERT:learned position embedding，是学习出来的。
>>Transformer:固定编码
>>BERT:随机初始化,然后训练
>>https://blog.csdn.net/weixin_42295205/article/details/106152707

正余弦函数
$$
PE(pos,2i)=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \\
PE(pos,2i+1)=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \\
其中，pos是位置，i是维度，也就是对每个维度都有一个sin或者cos计算
$$
>关于为什么transforme没有选择learned而是选择了fixed，论文只是说
>it **may** allow the model to extrapolate to sequence lengths longer than the ones encountered during training.
>看到这个may了吗，他也不知道哪个好，在实验中，两者差不多，只是觉得，生成式有助于外推到比训练数据更长的序列

>那为什么BERT用的是learned
>猜测是数据量足够大，可以任性地学习


#### WordPiece

BPE（Byte-Pair Encoding编码
把单词按字母分割，两两配对，统计字母pair出现的次数
每次将出现频率最高的相邻序列pair连接在一起。直到频率低于一定阈值
> w o r d:2  t h i r d:3 p o rd: 2
> r d 对出现得最多
> w o rd:2   t h i rd:3  p o rd:2
> ord 出现最多
> w ord:2    thi rd:3   p ord:2


#### 模型结构
>Transformer
>>1. A=scaled dot-product attention（缩放点乘注意力机制）
>>2. 多个A拼接，经过线性变换得到B multi-head attention
>>![enter description here](https://raw.githubusercontent.com/ZhaoKangkang0572/imgbed/master/小书匠/1604202231527.png)


>encoder attention 和 decoder attention,encoder-decoder的区别
>masked 就是这个意思，跟BERT不一样
>>在encoder侧，我们能一次获得全部的输入向量，所以可以做完全的scaled dot-product attention
>>但是在decoder，由于是机器翻译，我们只能从左到右输出翻译完成的句子，这时候完全的scaled dot-product attention是没法做的，例如此刻的输出是$w_i$，还没轮到$w_j,j>i$，我们并没有得到任何的$w_j$的向量，此时$score(v_i,j_i)$没法计算
>>方案是：$score(v_i,j_i)=-\infty$
>>这样的话$\alpha_{ij}=softmax(score(v_i,v_j))=softmax(-\infty)=0$
>
> encoder-decoder
>>Q来自decoder层，K和V都来自于encoder。类似于很多已经提出的seq2seq模型所使用的attention机制。


#### Feed forward-network
就是线性变换+Relu+线性变换
$$
FFN=max(0,xw_1+b_1)w_2+b_2
$$


#### Add&Norm

```python

class XXX(nn.module):
	def __init__():
		self.layer_norm = LayerNormalization(d_hid)
	def forward(self,x)
		residual = x
		return self.layer_norm(output + residual)
```
Add可以从图上看出来，是MHA,FFN处理前的数据，norm采用了layer-norm

#### 为什么要norm，并且采用layer，而不是batch
>首先BatchNorm这类归一化技术，目的就是让每一层的分布稳定下来，让后面的层可以在前面层的基础上安心学习知识。

>Batch Normalization 的处理对象是对一批样本， Layer Normalization 的处理对象是单个样本。Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。
![enter description here](https://raw.githubusercontent.com/ZhaoKangkang0572/imgbed/master/小书匠/1604205991193.png)
其实对自然语言处理来讲，本来就应该采用layer，因为batch不合理
NLP中，每个batch关联不大，且句子长度不一，强行归一化会损失信息间差异
而在句子间归一化，考虑到每个句子间是有关联的，内部差异信息进行一些损失，反而能降低方差



####  