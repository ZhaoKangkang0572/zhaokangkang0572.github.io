---
layout: post
title: SVM 
category: MLTheory
tags: SVM 推导 公式
description: 手撕SVM
---
* content
{:toc}

#### 总览

#### 推导
令超平面为：  $w^Tx+b=0$  
在分类时，支持向量与超平面的关系：

$$
\begin{cases} 
w^Tx_i+b=r &  \quad \, y_i = 1\\
w^Tx_i+b=-r & \quad \, y_i = -1
\end{cases}
$$

等式左右相乘
得到

$$
y_i(w^Tx_i+b)=r
$$

这时我们得到了函数间隔，也就是支持向量到超平面的距离

$$
r=w^Tx+b
$$

我们令 

$$
\|w\|=1
$$
来得到几何间隔(因为函数间隔中w,b会等比例缩放)：
$$
\frac{r}{\|w\|}=\frac{w^Tx}{\|w\|}+\frac{b}{\|w\|}
$$
根据SVM的要求，我们需要使得$\frac{r}{\|w\|}$最大
$$
\max \limits_{w,b}\frac{r}{\|w\|}
$$
等同于

$$
\min \limits_{w,b}\frac{\|w\|}{2} \\
s.t. \quad y_i(w^Tx_i+b) \geq 1
$$

优化带约束的极值问题，我们选用拉格朗日
引入拉格朗日乘子，$\alpha_{i} \geq0$，可以将有$d$个变量和$k$个约束条件的优化问题转变为有$d+k$个变量的无约束问题
得到
$$
\min \limits_{w,b}\frac{\|w\|}{2}+\sum \limits_{i=1}^{n}\alpha_{i}(1-y_i(w^Tx_i+b))
$$
得到KKT条件

$$
KKT=
\begin{cases} 
\alpha_i \geq0 \\
1-y_i(w^Tx_i+b) \leq 0 \\
\alpha(y_i f(x_i)-1)=0

\end{cases}
$$

得到转化后的式子

$$
\min \limits_{w,b} \max \limits_{\alpha} \frac{\|w\|}{2}+ \sum _\limits{i=1}^{n} \alpha_i(1-y_i(w^Tx_i+b_i))
$$
利用拉格朗日对偶原理，转化得到

$$
F=\max \limits_{\alpha} \min \limits_{w,b} \frac{\|w\|}{2}+ \sum _\limits{i=1}^{n} \alpha_i(1-y_i(w^Tx_i+b_i))
$$

对$w,b,\alpha$求导

$$
\begin{cases}
\frac{\partial F}{\partial w}=w-\sum _\limits{i=1}^{n}\alpha_i y_ix_i \Longrightarrow w=\sum _\limits{i=1}^{n}\alpha_i y_ix_i\\
\frac{\partial F}{\partial \alpha}=\sum _\limits{i=1}^{n}\alpha_iy_i=0
\end{cases}
$$

带入原式，整理得到

$$
\max _\limits{\alpha} \sum _\limits{i=1}^{n}\alpha_i-\frac{1}{2}\sum _\limits{i=1}{n}\sum _\limits{j=1}{m}\alpha_i\alpha_jy_iy_jx_i^Tx_j \\
\sum _\limits{n}^{i=1}\alpha_iy_i=0
$$

使用SMO来解
1. 选取一对参数
2. 固定 $\alpha$向量的其他参数，将$\alpha_i,\alpha_j$代入上式表达求解得到更新后的$\alpha_i,\alpha_j$
3. SMO不断执行1，2两步，直到收敛
4. 因为有约束$\sum _\limits{n}^{i=1}\alpha_iy_i=0$,所以$\alpha_i,\alpha_j$的关系可以确定，也就是$\alpha_i\alpha_i+\alpha_j\alpha_j$是一个常数


