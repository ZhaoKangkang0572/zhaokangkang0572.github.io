---
layout: post
title: Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions
category: Extraction
tags: Paper Reading
description: extra entity Descriptions
---
@Author
Guoliang Ji, Kang Liu, Shizhu He, Jun Zhao

AAAI 2017

## 纯在的问题
1. 无法很好地选择有效实例
2. 缺少有关实体的背景信息

## 解决方法
本文使用一个sentence-level attention模型来选择有效示例， 这个模型可以有效利用知识库中的有监督信息   
此外，作者们从freebase和维基百科页面抽取实体描述信息来补充背景知识。


## 本文任务
在多示例学习中，所有句子被一个triplet标记，然后组成一个包，每个句子即一个示例。假设现在有N个包 $\{B_1,B_2,\dots,B_N\}$ 都在训练集中， 第 $i$ 个包包含了 $q_i$ 个示例  $B_i=\{b^i_1,b^i_2,\dots,b^i_{q_i}\}$  $(i=1,\dots,N)$ 多示例学习的任务就是预测unseen bag的label， 我们需要基于这个训练集来学习一个关系抽取器，然后用它来预测测试数据。特别地，对于训练集中的包 $B_j=\{{b^j_1},{b^j_2},...,{b^j_{q_j}}\}$，我们需要从中抽取特征，然后训练分类器，对于测试集中的包，我们同样需要用同样的方法来抽取特征，然后使用分类器来预测给定的实体对。   
（白打了一堆废话）
## 疑问
<font color=#fff77> 1.sentence-level attention是如何选择示例的， 然后如何利用监督信息的   2.背景知识如何利用？</font>

## 方法
下图中所示，本文的神经网络结构包含了两个部份，PCNNs和sentence-level attention 模型
* PCNNs

  * Vector Representation   
  在神经网络中，需要将word token转换为低维向量，本文的word token指的是word和entity，下文统称为“word”。 本模型中，我们将word转换为向量，使用位置特征来特指给定实体对（也要转为向量）

    * 词向量  
    * 位置向量

  * 卷积层

  * Piecewise Max-pooling

* Sentence-levle Attention Module  

  * Attention layer



  * Entity Descriptions
  在这里，我们用另一个 传统CNN（一全卷操作层和一个max-pooling层）来从实体描述中抽取特征。   






![](../../graph/ArchitectureOfAPCNNs.png)



## 启发
我要的背景知识
