---
layout: post
title: HMM
category: DLTheory
tags: HMM
description: 为了全面理解HMM,比如HMM为什么是这样的，为什么要对联合概率建模
---

* content
{:toc}
#### 概率计算问题
给定模型 $\lambda = (A, B, \pi)$和观测序列$O=\{o1, o2, o3 ...\}$，计算模型λ下观测O出现的概率$P(O \| \lambda)$

==重点！！！！记住HMM是生成式模型，我们要求的是通过观测序列和隐藏状态的联合概率$P(O,I_i \|\lambda)$来求$P(O \| \lambda)$ #FF9800==

如果是判别式模型就直接求$P(O \| \lambda)$了，不需要那么多一大坨

- 怎么求
因为我们不知道隐藏状态的概率分布，所以
1. 列举所有可能的长度为T的状态序列$I = {i_1, i_2, ..., i_T}$
2. 求各个状态序列$I_i$与观测序列 的联合概率$P(O,I_i \|\lambda)$
3. 通过所有可能的状态序列求和$\sum _i P(O,I_i \|\lambda)$得到$P(O\|\lambda)$
步骤
>- 最终目标是求O和I同时出现的联合概率，即：
>$P(O,I\|\lambda)= P(O\|I,\lambda)P(I\|\lambda)$
>>- 找出所有可能的状态序列$I_i$
>>- 求$P(I_i\|\lambda)$
>>- 求$P(O\|I_i,\lambda)$，即对固定的(上一步得到的)状态序列$I_i$，观测序列O的概率
>>- $P(O,I_i\|\lambda)= P(O\|I_i,\lambda)P(I_i\|\lambda)$ 
>- 最后对所有可能的状态序列求和
>$P(O\|\lambda)= \sum _i P(O,I_i\|\lambda)$

#### 学习问题
学习问题分两种：

- 观测序列和隐状态序列都给出，求HMM。这种学习是监督学习。这就很简单，直接用极大似然估计来估计隐马尔可夫模型的初始状态概率Pi，状态转移概率A和观测概率B
- 给出观测序列，但没给出隐状态序列，求HMM。这种学习是非监督学习，利用Baum-Welch(鲍姆-韦尔奇)算法。


#### 预测问题（序列标注）
解码问题：已知模型$\lambda= (A, B, π)$和观测序列$O=\{o1, o2, o3 ...\}$，求给定观测序列条件概率$P(I \| O，\lambda)$最大的状态序列I
用维特比算法






















#### 马尔可夫模型是一种序列标注算法，从序列标注的角度理解
网上的很多博客，李老师的统计学都举的是盒子里取球的例子，我理解了HMM算法的计算推导，但是还是有很多“为什么”，这里用一个序列标注做例子，借此梳理一下
> 例  
> 给（马尔可夫）标注（a,b,c,d）  
>
> 首先我们描述问题：给定观测序列（句子序列）$(o_1,o_2,o_3,o_4)$,求令条件概率 $P(o_1,o_2,o_3,o_4\|s_1,s_2,s_3,s_4)$达到最大值的那个标注序列（隐藏状态）  
> 换句话说，在什么样的标注序列的条件上，给定的句子（观测序列）的概率是最大的  
> 什么样的$(s_1,s_2,s_3,s_4)$使得$(o_1,o_2,o_3,o_4)$概率最大  
>> 这里出现了第一个为什么，我们明明是已知句子序列，不应该是求什么样的标注序列概率最大吗？也就是不能直接求$P(I\|O)$吗？  
>> 假如说，我们直接求$max P(I\|O)$,那么我们要计算的就是 $P(I_1\|O_1)P(I_2\|O_2)P(I_3\|O_3)P(I_4\|O_4)$ $P(I_i\|O_i)$就是某个汉字所对应最可能的标注，但是这样的话，对于同一个汉字，统计出来最可能的标注是唯一的，显然这么做是不对的  
>> 进一步，我们加入状态转换，计算$P(I_1\|O_1)P(I_2\|O_2)P(I_2\|I_1)P(I_3\|O_3)P(I_3\|I_2)P(I_4\|O_4)P(I_4\|I_3)$ 加入一个状态转换$P(I_i\|I_{i_1})$
>> 举例就是计算$P(a\|马)P(b\|尔)P(b\|a)P(c\|可)P(c\|b)P(d\|夫)P(d\|c)$ 这就回到了HMM，HMM做的就是这个，为了计算这个，HMM要统计**表现矩阵和状态转移矩阵**，接下来跳过下面的猜测直接看看HMM的计算过程  

> 我的猜测是： 
从全概率公式和贝叶斯公式的疑问  
$$P(A,B)=P(A\|B)*P(B) $$很好理解，AB同时发生的概率等于B发生的概率乘上B发生时A也发生的概率  
我们要求的 $$ P(O\|I) $$ ，根据贝叶斯公式$$ P(O\|I)=\frac{P(I\|O)*(P(I))}{P(O)} $$   
如果根据全概率公式：$$ P(O\|I)=\frac{P(I,O)}{P(O)} $$ 不是也可以吗  

#### HMM计算过程
#### 概率计算
给定$\lambda=[\pi,A,B]$(初始状态概率分布，观测概率矩阵，状态转移概率矩阵)，求一个给定的观测序列X的概率，也就是$P(X\|\lambda)$
这个很好算，前向后向都可以

#### 学习问题
给定观测序列，求$\lambda=[\pi,A,B]$
