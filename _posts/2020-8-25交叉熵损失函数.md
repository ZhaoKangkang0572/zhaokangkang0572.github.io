---
layout: post
title: 交叉熵损失函数
category: MLTheory
tags: 损失函数
description: 交叉熵损失函数（sigmoid推广到softmax）
---

#### 二分类
当前标签为1的概率
>$\hat{y}=P(y=1|x)$
当前标签为0的概率
>$1-\hat{y}=P(y=0|x)$

从极大似然的角度整合
>$P(y|x)=\hat{y}^y(1-\hat{y})^{1-y}$

引入对数
>$log(P(y|x))=log(\hat{y}^y(1-\hat{y})^{1-y})=ylog\hat{y}+(1-y)log(1-\hat{y})$

引入损失函数,这就是单样本的损失函数
>$L=-log(p(y|x))=-ylog\hat{y}-(1-y)log(1-\hat{y})$

N个样本的总损失函数，只需累加
>$L=\sum_{i=1}^N{-y^{(i)}log\hat{y}^{(i)}-(1-y^{(i)})log(1-\hat{y}^{(i)})}$

$\hat{y}$ 预测值 $y$实际值，取0，1

#### 引入sigmoid的形式
现在假设正类为+1，负类为-1
sigmoid具有如下性质
>$1-g(s)=g(-s)$
>当sigmoid的输出为1时，标签也为1，
>所以当$y=1$时，$P(y=1|x)=g(s)$成立
>当$y=-1$时，$P(y=-1|x)=1-P(y=1|x)=1-g(s)=g(-s)$
>整合一下
>不论y是+1还是-1，都有$P(y|x)=g(ys)$
>引入log函数，要让其概率最大，只需要让其负数最小即可,所以定义损失函数
>$L=-log(g(ys))$
>
代入sigmoid表达式,得到单个样本的交叉熵损失函数
>$L=-log\frac{1}{1+e^{-ys}}=log(1+e^{(-ys)})$
N个样本的交叉熵损失函数为累加

>$L=\sum{log(1+e^{(-ys)})}$
Reference:
https://blog.csdn.net/red_stone1/article/details/80735068
https://www.cnblogs.com/lijie-blog/p/10166002.html