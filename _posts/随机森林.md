---
title: 随机森林 
tags: 新建,模板,小书匠
renderNumberedHeading: true
grammar_cjkRuby: true
---


释义：随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。


算法流程：
1. 在N个样本里面，有放回地随计选择N个样本，作为决策树根节点处的样本
2. 分裂：在每个样本的M个属性中选择m个属性，满足m<<M, 采取一种策略（比如信息增益）来选择m中的某个属性作为该节点的分裂属性


优点：
a. 在数据集上表现良好，两个随机性的引入，使得随机森林不容易陷入过拟合
>首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M个feature中，选择m个（m << M）。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。一般很多的决策树算法都一个重要的步骤——剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。

b. 在当前的很多数据集上，相对其他算法有着很大的优势，两个随机性的引入，使得随机森林具有很好的抗噪声能力
c. 它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化
d. 可生成一个Proximities=（pij）矩阵，用于度量样本之间的相似性： pij=aij/N, aij表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数
e. 在创建随机森林的时候，对generlization error使用的是无偏估计f. 训练速度快，可以得到变量重要性排序（两种：基于OOB误分率的增加量和基于分裂时的GINI下降量
g. 在训练过程中，能够检测到feature间的互相影响h. 容易做成并行化方法i. 实现比较简单

作者：许铁-巡洋舰科技
链接：https://zhuanlan.zhihu.com/p/22097796
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。