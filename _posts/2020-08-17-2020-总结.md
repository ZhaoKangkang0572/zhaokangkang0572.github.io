---
layout: post
title: 总结
category: DLTheory
tags: 面试 
description: 
---

* content
{:toc}

#### 朴素贝叶斯
>朴素贝叶斯=贝叶斯定律+条件独立
 ##### 何为朴素
所有的变量相互独立
$$
P(Y|X_1, X_2) = \frac{P(X_1|Y) P(X_2|Y) P(Y)}{P(X_1)P(X_2)}
$$
#####  朴素贝叶斯中的三种模型
1. 多项式模型
适用于离散特征情况将重复的词语视为其出现多次。
2. 伯努利模型
适用于离散特征情况，它将重复的词语都视为只出现一次
 $$P( " 代开“， ”发票“， ”发票“， ”我“ | S) \\
 = P("代开" | S)   P( ”发票“ | S) P("我" | S)$$
3. 高斯模型
适合连续特征

#####  朴素贝叶斯分类中某个类别的概率为0怎么办？
 拉普拉斯平滑：给概率为零的类别加$\lambda$   可以取1也可以$1>=\lambda>=0$
 
 #####  优缺点
 优点： 对小规模数据表现很好，适合多分类任务，适合增量式训练。
 缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）
 
 #####  朴素贝叶斯与 LR 区别？
 生成or判别
 有无假设
 使用场景（数据集大小）
 
 ### 损失函数
  ##### 0-1 损失函数
  ##### 绝对值损失函数
  ##### 平方损失函数
  ##### 对数损失函数
  ##### 指数损失函数
  ##### hinge损失函数
  
  ### 代价函数
  ##### 二次代价函数
  不适用于分类, 分类也可以，但是均方在求导时要对sigmoid求导，sigmoid的导数范围在0，1两端非常小，导致参数学习很慢
  
  #### 为什么回归使用平方差损失函数 分类使用交叉熵
因为分类都用softmax

#### Logistic 回归和==极大似然估计 #FF9800==和==对数损失函数 #FF9800==和==交叉熵代价函数 #FF9800==的关系
首先明确，**==LR模型的输出是样本属于某类的概率 #FF9800==**
假设我们有一组初始化LR的参数$\theta=\{w,b\}$
给定训练集$data=((x_i,y_1),(x_2,y_2)...)$
对于每个$x_i$，我们都能根据LR算出结果是$y_i$的概率
>使用极大似然估计有一个前提，就是你要知道他的概率密度函数,逻辑回归的概率密度函数：sigmoid

对于所有样本来说，每个都被正确分类的概率就是所有的LR输出值的连乘
$$  L(\theta)=\prod_{i=1}^{m} P(y=1|x_i)^{y_i}P(y=0|x_i)^{1-y_i} $$
梯度下降法要求凸函数来保证得到全局最优解，而极大似然法得到的函数，为非凸函数，因此转换为，对数似然函数：
 $$   \ln L(\theta)=\sum_{i=1}^{m}[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln{P(y=0|x_i)}]\\
    =\sum_{i=1}^m[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln(1-P(y=1|x_i))]$$
然后我们可以直接用极大似然估计来找到最好的参数$\theta$


==到这儿不就结束了吗，为什么我们还要引入损失函数来优化 #FF9800==
>提供几个视角

>具体看这里 https://baijiahao.baidu.com/s?id=1611678624768980723&wfr=spider&for=pc

>深度学习和逻辑回归之所以在参数设置上有这样的不同（参数是否可以初始化为0），是因为二者的损失函数构造不同，深度学习中是将原始特征加权后进行了激活函数的非线性变换从而构造出了最后的损失函数，而逻辑回归是将损失函数构造了最大似然估计https://zhuanlan.zhihu.com/p/149183964
>>是不是说如果只有极大似然估计的话，只能求具体参数而不能调整
>>而损失函数可以调整参数

>https://zhuanlan.zhihu.com/p/27719875



我们可以发现，这个对数似然函数与对数损失函数形式上一样

sigmoid一般用 交叉熵
softmax一般用对数似然代价函数
对数似然代价函数在二分类时可以化简为交叉熵代价函数的形式。

#### 面试时这么回答
作者：跪求offer🙏🙏🙏
链接：https://www.nowcoder.com/discuss/485589?type=1
来源：牛客网

MSE + sigmoid、交叉熵 + sigmoid配套。后者计算少1个sigmoid的导数项，梯度等于预测值和真实值差值乘以x，计算更方便，而且差值越大代表梯度越大，更新越快，符合物理意义；前者sigmoid导数取值范围为[0,1]，计算复杂，而且可能出现梯度消失

### 过拟合
但是为什么可以防止过拟合呢？
由损失函数可知，加入了正则项以后，其实本质是w前面的系数减小了，也即权重衰减（weight decay）。W变小了，为什么就可以防止过拟合呢？权值w减小，使网络中的某些权重尽可能为0，也就相当于减小了网络复杂度，同时对数据有更好的容错能力（泛化能力）防止过拟合。

#### L2正则化
L2 正则化公式非常简单，直接在原来的损失函数基础上加上权重参数的平方和：
$$L=E_{in}+\lambda \sum \limits_{i}w_{i}^{2}$$
Ein 是未包含正则化项的训练样本误差，λ 是正则化参数
- 为什么
过拟合就是模型复杂
模型复杂意味着参数过多
最简单的办法就是约束参数个数 也就是等于给一些参数设0$w=0$
但是这样很难属于NP hard
所以就对参数的平方和做限制，越小就越简单（都为0最简单~）

#### L1Z正则化
直接在原来的损失函数基础上加上权重参数的绝对值：
$$L=E_{in}+\lambda \sum \limits_{i}|w|$$

#### L1和L2正则化的区别
- L1正则化会得到稀疏解，可用作特征选择。
- L2正则化会得到趋于0的解，起到防止过拟合的作用。
可以通过对w求导来分析
L1:求导后 w在经过多次迭代后，总会使得 $w=0$，从而 l1 正则化会得到稀疏解。
L2:求导后这一项有w这个变量，在梯度下降过程中起到减速作用。使得w在经过多次迭代后，w会趋近于0，而不会等于0，从而L2正则化会得到趋于0的解。

>推导
>https://blog.csdn.net/m0_38045485/article/details/82147817?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param
>
>L1正则化可通过假设权重w的先验分布为拉普拉斯分布，由最大后验概率估计导出。
L2正则化可通过假设权重w的先验分布为高斯分布，由最大后验概率估计导出。

#### dropout
防止参数过分依赖训练数据，增加参数对数据集的泛化能力。
- RNN
放在输入->RNN与RNN->输出的位置，也就是RNN整个循环的前后，RNN循环里面不放

- CNN
全连接之后

#### 区别
正则化方法是通过修改损失函数来提高过拟合能力的，
dropout是通过改变网络的结构来提高的，这是和正则化方法最本质的区别。


#### 场景
理论上讲，参数如果服从高斯分布就用l2，拉普拉斯分布就用l1。实际上你也不知道参数该服从什么分布，所以一般如果你需要稀疏性就用l1，比如参数量很大情况，一般不单独用l2吧，可以l1+l2，不过最终还是看效果…哪个好就用哪个…另外一般框架使用l1，可能也不能保证稀疏性，取决于底层实现…





### 决策树
信息熵$H(X)=-\sum \limits_{i=1}^{n}p_i \ln pi$
条件熵$H(D|A)$
信息增益：$H(X)-H(D|A)$
#### ID3
当数据集有多个特征，ID3决策树的过程
https://blog.csdn.net/gyq423/article/details/81946089
ID3是选择信息增益最大的特征来划分子集，重点是信息增益指的是哪两者的比较？
首先计算对于label的信息熵，也就是买和不买
然后从第一个特征开始，计算在这个特征的条件下，买和不买的信息熵
信息增益就是这两者的差

为什么ID3不适用于回归
假设预测买东西 我们有特征年龄【小孩，中年，老年】，在根据信息增益选择了特征之后，可以直接给数据集分类
但如果给定特征【1岁，2岁，3岁，...，100岁】
我们的信息增益直接计算就变得非常麻烦，
回想一下条件信息熵，我们在计算信息增益的时候要先计算条件信息熵
计算条件信息熵，就是固定住该特征的每个取值再计算信息熵，然后求和，
对于【小孩，中年，老年】来说，我们分别固定这三个特征取值就行
但是对于【1岁，2岁，3岁，...，100岁】，我们要固定这一百个取值，再扩大一点对数数值[1.231,1.12343234,1234353......]这种，计算就很麻烦，而且没有必要了