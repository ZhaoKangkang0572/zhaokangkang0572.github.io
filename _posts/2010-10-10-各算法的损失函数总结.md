---
layout: post
title: 各算法的损失函数总结
category: MLTheory
tags: SVM
description: 
---
* content
{:toc}

#### SVM
1. 硬间隔
 **对于硬间隔来说，目标是完全分类的，不需要损失函数**
 目标有两个
 >1. 最大化间隔  $\min \limits_{w,b} \frac{1}{2}\|w\|^2$
 >2. 约束条件（使样本被正确分类） $s.t. y_i(w^Tx_i+b) \geq 1$
这时候只要转化问题，使用SMO算法求解

2. 软间隔
引入松弛变量之后，目标函数变为
$$
\min \limits_{w,b} \frac{1}{2}\|w\|^2+ \sum \limits_{i=1}^{n}\xi_i
$$
通常分类问题可以用$(0,1)$损失函数，但是他不是连续的，求导不好求，所以使用合页函数
$$
l_{hinge}=max(0,1-z)
$$
所以软间隔的目标函数为：
$$
\min \limits_{w,b} \frac{1}{2}\|w\|^2+ \sum \limits_{i=1}^{n}\xi_i \\
s.t. y_i(w^Tx_i+b) \geq 1-\xi_i \\
$$
其中
$$
\xi=max(0,1-y_i(w^Tx_i+b))
$$
之后同样转化问题，使用SMO算法求解


#### Adaboost
**Adaboost的损失函数只能用指数损失函数，换了其他的就不叫adaboost
或者说，boosting + 指数损失= Adaboost**

Adaboost可以直接使偏导为0来求解，不需要梯度下降

由于采用分步向前：  
第k-1轮的强学习器:
$$
f_{k-1}(x)=\sum_{i=1}^{k-1}\alpha_i G_i(x)
$$
第k轮的强学习器:
$$
f_{k}(x)=\sum_{i=1}^{k}\alpha_i G_i(x)
$$
可以得到

$$
f_k(x)=f_{k-1}(x)+\alpha_k G_k(x)
$$

引入指数损失函数  

对于每一轮
$$
\arg \min \limits_{\alpha,G}\sum_{i=1}^{n}\exp -(y_if_i(k))
$$
总损失函数（分步向前）
$$
\alpha_k,G_k(x)=\arg \min \limits_{\alpha,G}\sum_{i=1}^{n}\exp (-y_{i-1})[f_{i-1}(k)+\alpha G(x)]
$$


Reference:
https://www.jianshu.com/p/00a405962dca?from=singlemessage

