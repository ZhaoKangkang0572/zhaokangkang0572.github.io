---
layout: post
title: 逻辑回归 
category: DLTheory
tags: 逻辑回归
description: 
---
* content
{:toc}


#### 什么是逻辑回归
>逻辑回归是一种广义的线性模型
>线性回归使用最小二乘法作为参数估计方法，逻辑回归使用极大似然法作为参数估计方法
>logistic回归是分析**因变量取某个值的概率与自变量的关系**，而线性回归是**直接分析因变量与自变量的关系**
$h_\theta(x) = sigmoid(\theta^T X)  = \frac{1}{1 + e^{-\theta^T X}}$

#### 逻辑回归为什么要用sigmoid
>一种回答就是sigmoid具有巴拉巴拉的性质，主要是使得线性函数$\theta^T X$的值经过sigmoid后呈s曲线，$\theta^T X$越接近正无穷，映射后越接近1，反之越接近0,方便分类
但是其实，仔细分析逻辑回归模型，会发现，**逻辑回归模型并不等于sigmoid**，这点很重要，很多时候我看一些博客问答都会把这两者混淆

>准确来说，逻辑回归模型=线性模型($\theta^T X$)+sigmoid+变换函数$f(x)=
\begin{cases}
1,& Y>0.5\\
0,& Y<=0.5
\end{cases}$
>实际上在训练阶段，为了优化$\theta$，还需要最后接一个一个损失函数

所以说，只要符合要求，或者说能实现二分类，这三个都可以换掉，sigmoid也可以换成任意一个具有非线性性质并且堆成的函数，比如说
![enter description here](https://raw.githubusercontent.com/ZhaoKangkang0572/imgbed/master/小书匠/1598515122200.png)