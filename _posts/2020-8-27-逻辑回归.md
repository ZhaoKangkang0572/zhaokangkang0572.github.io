---
layout: post
title: 逻辑回归 
category: DLTheory
tags: 逻辑回归
description: 
---
* content
{:toc}


#### 什么是逻辑回归
==**写在前面，我们考虑逻辑回归时，先要知道，我们是假设独立观测值$\hat{y}$符合伯努利分布，所以搞了个分类模型，叫逻辑回归，这样的话，其实下个问题为什么用sigmoid就迎刃而解了**==
>逻辑回归是一种广义的线性模型
>线性回归使用最小二乘法作为参数估计方法，逻辑回归使用极大似然法作为参数估计方法
>logistic回归是分析**因变量取某个值的概率与自变量的关系**，而线性回归是**直接分析因变量与自变量的关系**
$h_\theta(x) = sigmoid(\theta^T X)  = \frac{1}{1 + e^{-\theta^T X}}$

#### 逻辑回归为什么要用sigmoid
>一种回答就是sigmoid具有巴拉巴拉的性质，主要是使得线性函数$\theta^T X$的值经过sigmoid后呈s曲线，$\theta^T X$越接近正无穷，映射后越接近1，反之越接近0,方便分类
但是其实，仔细分析逻辑回归模型，会发现，**逻辑回归模型并不等于sigmoid**，这点很重要，很多时候我看一些博客问答都会把这两者混淆

>准确来说，逻辑回归模型=线性模型($\theta^T X$)+sigmoid+变换函数$f(x)=
\begin{cases}
1,& Y>0.5\\
0,& Y<=0.5
\end{cases}$
>实际上在训练阶段，为了优化$\theta$，还需要最后接一个一个损失函数

所以说，只要符合要求，或者说能实现二分类，这三个都可以换掉，sigmoid也可以换成任意一个具有非线性性质并且堆成的函数，比如说
![enter description here](https://raw.githubusercontent.com/ZhaoKangkang0572/imgbed/master/小书匠/1598515122200.png)


#### 损失函数

##### 交叉熵损失函数
从极大似然的角度整合
>$P(y|x)=\hat{y}^y(1-\hat{y})^{1-y}$

引入对数
>$log(P(y|x))=log(\hat{y}^y(1-\hat{y})^{1-y})=ylog\hat{y}+(1-y)log(1-\hat{y})$

引入损失函数,这是单样本的损失函数,我们选择添加负号来最小化损失函数（其实本质上都是一样的。最小化损失函数，就用梯度下降，最大化似然函数，就用梯度上升）
>$L=-log(p(y|x))=-ylog\hat{y}-(1-y)log(1-\hat{y})$

N个样本的总损失函数，只需累加
>$L=\sum_{i=1}^N{-y^{(i)}log\hat{y}^{(i)}-(1-y^{(i)})log(1-\hat{y}^{(i)})}$


$\hat{y}$ 预测值 $y$实际值，取0，1

求下导
>$\hat{y}=g(x^i)$ $h_{\theta}(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}$

![交叉熵函数求导](https://raw.githubusercontent.com/ZhaoKangkang0572/imgbed/master/小书匠/1598522167354.png)
##### 为什么用交叉熵损失函数

这里谈一谈为什么用交叉熵损失函数
1. 一般来说，回归任务直接用最小二乘法损失函数就行，但是这是分类任务，我们的标签是离散的，使用交叉熵损失时，真实标签为1，其余为零，也就是我们不关心是0.7还是0.8，只要大于0.5，就按1来算

**所以说交叉熵损失函数是接在变换函数之后的**
2. 更重要的一个原因是，还记得sigmoid的图像吗，极大极小值对应的变换函数输出是逼近0，1，所以会造成梯度消失，但是交叉熵损失函数不一样，log抵消了exp（具体看推导）