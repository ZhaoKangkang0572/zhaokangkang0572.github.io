---
layout: post
title: 决策树 
category: DLTheory
tags: 决策树
description: 
---
* content
{:toc}



#### 总览：
1. 每个节点代表一个属性的判断
2. 每个分支代表判断结果的输出
3. 每个叶节点代表一种分类结果





#### ID3
由增熵（Entropy）原理来决定那个做父节点，那个节点需要分裂。对于一组数据，熵越小说明分类结果越好。熵定义如下：

$Entropy=-\sum[p(x_i)*log(p(x_i))]$
p()是某一类的概率
在二分类的情况下，两类概率都占50%
$Entropy=-(0.5*log(0.5)+0.5*log(0.5))=1$
在只有一类的情况下
$Entropy=-(1*log_2(1)+0)=0$

所以对两组输出
$[0.8,0.1,0.1]$
$[1,0,0]$
第二组中标签1位置的概率为1，这时候熵最小，分类效果最好

为1的时候，是分类效果最差的状态
为0的时候，是完全分类的状态。
熵的不断最小化，实际上就是提高分类正确率的过程。

>**为什么说类似$[1,0,0]$是最好的呢，万一结果是$[0，1,0]$**
注意，这跟softmax输出不一样，这里的决策树是一个属性选择的过程，也就是说，我们通过选择属性，使得结果为训练集中正确答案$[1,0,0]$，

例：
根据属性A将数据划分为$D={D_1,D_2,...,D_n}$个子集
1. 计算熵(属性A的)
$H_A(D)=\sum \frac{|D_i|}{|D|}H(D_i)$
2. 计算信息增益（根据A划分前和划分后的）
$Gain(A)=H(D)-H_A(D)$
#### C4.5
1.计算分母
$split(D)=-\sum \frac{|D_j|}{|D|}log(\frac{|D_j|}{|D|})$

$GainRatio(A)=\frac{Gain(A)}{split(A)}$



#### CART

Gini指标

$Gini(D)=1-\sum p_i^2$