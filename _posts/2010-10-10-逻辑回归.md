---
layout: post
title: 逻辑回归 
category: DLTheory
tags: 逻辑回归
description: 
---
* content
{:toc}


#### 什么是逻辑回归
==**写在前面，我们考虑逻辑回归时，先要知道，我们是假设独立观测值$\hat{y}$符合伯努利分布，所以搞了个分类模型，叫逻辑回归，这样的话，其实下个问题为什么用sigmoid就迎刃而解了**==
>逻辑回归是一种广义的线性模型
>线性回归使用最小二乘法作为参数估计方法，逻辑回归使用极大似然法作为参数估计方法
>logistic回归是分析**因变量取某个值的概率与自变量的关系**，而线性回归是**直接分析因变量与自变量的关系**
$h_\theta(x) = sigmoid(\theta^T X)  = \frac{1}{1 + e^{-\theta^T X}}$

#### 逻辑回归为什么要用sigmoid
>一种回答就是sigmoid具有巴拉巴拉的性质，主要是使得线性函数$\theta^T X$的值经过sigmoid后呈s曲线，$\theta^T X$越接近正无穷，映射后越接近1，反之越接近0,方便分类
但是其实，仔细分析逻辑回归模型，会发现，**逻辑回归模型并不等于sigmoid**，这点很重要，很多时候我看一些博客问答都会把这两者混淆

>准确来说，逻辑回归模型=线性模型($\theta^T X$)+sigmoid+变换函数$f(x)=
\begin{cases}
1,& Y>0.5\\
0,& Y<=0.5
\end{cases}$
>实际上在训练阶段，为了优化$\theta$，还需要最后接一个一个损失函数

所以说，只要符合要求，或者说能实现二分类，这三个都可以换掉，sigmoid也可以换成任意一个具有非线性性质并且堆成的函数，比如说
![enter description here](https://raw.githubusercontent.com/ZhaoKangkang0572/imgbed/master/小书匠/1598515122200.png)


#### 损失函数

##### 为什么损失函数是交叉熵损失函数，但是我们要最小化负似然函函数
首先给出结论，最小化交叉熵可以使的我们要求的预测的概率分布接近真正的概率分布
从形式上看，最小化交叉熵和最小化负对数似然函数是等价的
>**交叉熵**
>我们设$P(x)$为真实分布，$q(x)$为非真实分布，也就是我们预测的分布
>>信息量
>>信息量的大小与信息发生的概率成反比。概率越大，信息量越小。概率越小，信息量越大
>>设某一事件发生的概率为P(x)，其信息量表示为
>>$I(x)=-\ln P(x)$
>信息熵
>>信息熵也被称为熵，用来表示所有信息量的期望
>>也就是每个事件发生的概率乘上这个事件的信息熵的总和
>>$H(x)=-\sum_{i=1}^nP(x_i)\ln P(x_i) X=x_1,x_2,...,x_n$ 
>同一随机变量X,我们有真实概率分布P(X)和预测的Q(x),这时我们用一个KL散度用以描述这两个分布之间的差异：
>$D_{KL}(P,Q)=\sum_{i=1}^n[\ln P(x_i)-\ln Q(x_i)]$
>交叉熵
>就是每个事件真实发生的概率乘上这个事件的预测的信息熵的总和
>$H(X)=-\sum_{i=1}^n P(x_i)\ln q(x_i) X=x_1,x_2,...,x_n$ 
>对于2分类，该式等于
>$=-[P(x=0)\ln Q(x=0)+P(x=1)\ln Q(x=1)]$
>$=1[p\ln q +(1-p)\ln (1-q)]$
>=$-[y\ln H_\theta (x)+(1-y)\ln(1-h_\theta(x))]$
>对于所有训练样本，我们有
>$-\sum [y^i\ln H_\theta (x^i)+(1-y^i)\ln(1-h_\theta(x^i))]$
>为什么要最小化交叉熵
>因为KL散度衡量了两个分布的差异，KL散度越小差异越小
>而==KL散度 = 交叉熵 - (真实分布的)信息熵==, 在训练时，输入数据与标签常常已经确定，那么真实概率分布$P(x)$也就确定下来了，所以信息熵就是一个常量
从极大似然的角度整合
>$P(y|x)=\hat{y}^y(1-\hat{y})^{1-y}$

引入对数
>$log(P(y|x))=log(\hat{y}^y(1-\hat{y})^{1-y})=y \log \hat{y}+(1-y)log(1-\hat{y})$

引入损失函数,这是单样本的损失函数,我们选择添加负号来最小化损失函数（其实本质上都是一样的。最小化损失函数，就用梯度下降，最大化似然函数，就用梯度上升）
>$L=-log(p(y|x))=-y \log \hat{y}-(1-y)log(1-\hat{y})$

N个样本的总损失函数，只需累加
>$L=\sum_{i=1}^N{-y^{(i)}log\hat{y}^{(i)}-(1-y^{(i)})log(1-\hat{y}^{(i)})}$


$\hat{y}$ 预测值 $y$实际值，取0，1

求下导
>$\hat{y}=g(x^i)$ $h_{\theta}(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}$

![交叉熵函数求导](https://raw.githubusercontent.com/ZhaoKangkang0572/imgbed/master/小书匠/1598522167354.png)
##### 为什么用交叉熵损失函数

这里谈一谈为什么用交叉熵损失函数
1. 一般来说，回归任务直接用最小二乘法损失函数就行，但是这是分类任务，我们的标签是离散的，使用交叉熵损失时，真实标签为1，其余为零，也就是我们不关心是0.7还是0.8，只要大于0.5，就按1来算

**所以说交叉熵损失函数是接在变换函数之后的**
2. 更重要的一个原因是，还记得sigmoid的图像吗，极大极小值对应的变换函数输出是逼近0，1，所以会造成梯度消失，但是交叉熵损失函数不一样，log抵消了exp（具体看推导）