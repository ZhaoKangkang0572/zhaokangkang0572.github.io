---
title: 决策树
tags: 新建,模板,小书匠
renderNumberedHeading: true
grammar_cjkRuby: true
---

https://www.zhihu.com/search?type=content&q=%E5%86%B3%E7%AD%96%E6%A0%91

释义：决策树是一种树形结构，其中每个节点代表一个属性判断，每个分支代表判断结果的输出，最后每个叶节点代表一种分类结果


算法描述：
1. 节点的分裂：一般当一个节点所代表的属性无法给出判断时，则选择将这一节点分成2个子节点（如不是二叉树的情况会分成n个子节点）

2. 阈值的确定：选择适当的阈值使得分类错误率最小 （Training Error）。

常用决策树
1. ID3
2. C4.5
3. CART(Classification And Regression Tree)

#### ID3
由增熵（Entropy）原理来决定那个做父节点，那个节点需要分裂。对于一组数据，熵越小说明分类结果越好。熵定义如下：

$Entropy=-\sum[p(x_i)*log(p(x_i))]$
p()是某一类的概率
在二分类的情况下，两类概率都占50%
$Entropy=-(0.5*log(0.5)+0.5*log(0.5))=1$
在只有一类的情况下
$Entropy=-(1*log_2(1)+0)=0$

所以对两组输出
$[0.8,0.1,0.1]$
$[1,0,0]$
第二组中标签1位置的概率为1，这时候熵最小，分类效果最好

为1的时候，是分类效果最差的状态
为0的时候，是完全分类的状态。
熵的不断最小化，实际上就是提高分类正确率的过程。

>**为什么说类似$[1,0,0]$是最好的呢，万一结果是$[0，1,0]$**
注意，这跟softmax输出不一样，这里的决策树是一个属性选择的过程，也就是说，我们通过选择属性，使得结果为训练集中正确答案$[1,0,0]$，

例：
根据属性A将数据划分为$D={D_1,D_2,...,D_n}$个子集
1. 计算熵(属性A的)
$H_A(D)=\sum \frac{|D_i|}{|D|}H(D_i)$
2. 计算信息增益（根据A划分前和划分后的）
$Gain(A)=H(D)-H_A(D)$
#### C4.5
1.计算分母
$split(D)=-\sum \frac{|D_j|}{|D|}log(\frac{|D_j|}{|D|})$

$GainRatio(A)=\frac{Gain(A)}{split(A)}$



#### CART

Gini指标

$Gini(D)=1-\sum p_i^2$