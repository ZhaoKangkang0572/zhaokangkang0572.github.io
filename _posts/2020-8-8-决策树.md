---
title: 决策树
tags: 新建,模板,小书匠
renderNumberedHeading: true
grammar_cjkRuby: true
---

https://www.zhihu.com/search?type=content&q=%E5%86%B3%E7%AD%96%E6%A0%91

释义：决策树是一种树形结构，其中每个节点代表一个属性判断，每个分支代表判断结果的输出，最后每个叶节点代表一种分类结果


算法描述：
1. 节点的分裂：一般当一个节点所代表的属性无法给出判断时，则选择将这一节点分成2个子节点（如不是二叉树的情况会分成n个子节点）

2. 阈值的确定：选择适当的阈值使得分类错误率最小 （Training Error）。

常用决策树
1. ID3
2. C4.5
3. CART(Classification And Regression Tree)

#### ID3
由增熵（Entropy）原理来决定那个做父节点，那个节点需要分裂。对于一组数据，熵越小说明分类结果越好。熵定义如下：

$Entropy=-sum[p(x_i)*log_2(p(x_i))]$
p()是某一类的概率
在二分类的情况下，两类概率都占50%
$Entropy=-(0.5*log_2(0.5)+0.5*log_2(0.5))=1$
在只有一类的情况下
$ENtropy=-(1*log_2(1)+0)=0$

所以当Entropy最大为1的时候，是分类效果最差的状态，当它最小为0的时候，是完全分类的状态。因为熵等于零是理想状态，一般实际情况下，熵介于0和1之间。
熵的不断最小化，实际上就是提高分类正确率的过程。

比如上表中的4个属性：单一地通过以下语句分类： 1. 分数小于70为【不是好学生】：分错1个 2. 出勤率大于70为【好学生】：分错3个 3. 问题回答次数大于9为【好学生】：分错2个 4. 作业提交率大于80%为【好学生】：分错2个最后发现  分数小于70为【不是好学生】这条分错最少，也就是熵最小，所以应该选择这条为父节点进行树的生成，当然分数也可以选择大于71，大于72等等，出勤率也可以选择小于60，65等等，总之会有很多类似上述1~4的条件，最后选择分类错最少即熵最小的那个条件。而当分裂父节点时道理也一样，分裂有很多选择，针对每一个选择，与分裂前的分类错误率比较，留下那个提高最大的选择，即熵减最大的选择。

作者：犀利哥的大实话
链接：https://zhuanlan.zhihu.com/p/30059442
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。


#### C4.5

通过对ID3的学习，可以知道ID3存在一个问题，那就是越细小的分割分类错误率越小，所以ID3会越分越细，比如以第一个属性为例：设阈值小于70可将样本分为2组，但是分错了1个。如果设阈值小于70，再加上阈值等于95，那么分错率降到了0，但是这种分割显然只对训练数据有用，对于新的数据没有意义，这就是所说的过度学习（Overfitting）。

所以为了避免分割太细，c4.5对ID3进行了改进，C4.5中，优化项要除以分割太细的代价，这个比值叫做信息增益率，显然分割太细分母增加，信息增益率会降低。除此之外，其他的原理和ID3相同。



#### CART
