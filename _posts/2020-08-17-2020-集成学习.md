---
layout: post
title: 集成学习
category: 论坛
tags: 论坛 AI 
description: 
---

* content
{:toc}

偏差：描述的是**预测值的期望**与**真实值**之间的差距
方差：描述的是预测值的变化范围，离散程度，也就是**离其期望值的距离**
#### 离散型随机变量方差
期望：$E(X)=\sum \limits _{i=1}^n x_ip_i$
$p_i=\frac{1}{n}$时期望：$E(X)=\frac{\sum \limits_{i=1}^{n}x_i}{n}$
**方差是一种特殊的期望，表示X与平均值$E[X]$之间差异的平方的期望值**
方差：$D(X)=\sum \limits_{i=1}^{n}[x_i-E(X)]^2 p_i=E[X-E(X)]^2$
$D(X)=E(X^2)-[E(X)]^2$
证明,由于方差公式具有线性性质$E(ax+by+c)=aE(x)+bE(y)+c$ ，$E(X)$是常量
$D(X)=E[X-E(X)]^2 \\
=E[X^2+E(X)^2-2XE(X)] \\
=E[X^2]+E(X)^2-2E(X)E[X] \\
=E[X^2]-E(X)^2$

#### 准备
如果X,Y互相独立
$D(X+Y)=D(X)+D(Y)$
$D(CX)=E[(CX-E(CX))^2]=C^2E[(E-E(X))^2]=C^2D(X)$
#### 弱模型与强模型
1. 弱模型就是垃圾模型，准确率略大于50%，准确来说弱学习器定义是泛化性能弱,略优于随机猜测的学习器
2. 为什么弱模型偏差大，方差小，而强学习器偏差小，方差大
> 偏差大是因为它垃圾，距离真实标签远
> 方差小：比如学渣，再怎么考也就是在0-30之间，偏差自然很大（相对于100分），都是一二十分的，这些分数的方差肯定不大，但是学霸的能考100，也能考10分，尽管10分的概率很低，但是也有可能，只要出现了，就会极大地拉高方差
> 

#### 集成学习里面讨论基模型是强模型还是弱模型这个讨论本身是没有意义的，也是错误的
>首先集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性

>Boosting和bagging的区别应该是基学习器之间依赖关系的强弱，而不是学习器的强弱
>根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类
>1. 即个体学习器问存在强依赖关系、必须串行生成的序列化方法
>2. 以及个体学习器间不存在强依赖关系、可同时生成的并行化方法;前者的代表是 Boosting ，后者的代表是 Bagging 和"随机森林" (Random Forest).

>Boosting 
>是一族可将弱学习器提升为强学习器的算法，所以基模型为弱模型，看名字也知道（boosting，提升）
#### boosting
可以降低偏差
#### bagging
- bagging是减少方差
- 多个强学习器并行学习
- 一个随机：每个弱鸡得到的数据都是有放回的随机采样

#### 随机森林
- bagging的进阶版
- 两个随机：除了随机采样，增加一个随机特征选择
- 里面的弱学习器都是决策树

####
-为什么bagging可以降低方差而不能降低偏差
1. 子样本集的相似性以及使用的是同种模型，也导致bagging的子模型并不独立（有放回的取样本导致训练子集会重叠）
 所以 $E[\frac{\sum X_i}{n}]=E[X_i]$，也就是说bagging后的偏差接近单个子模型的偏差
 如果子模型完全独立，有
 
 
 #### Bagging和Boosting两者之间的区别
-  训练样本集
Bagging：训练集是有放回抽样，从原始集中选出的K组训练集是相互独立的。
Boosting：每一次迭代的训练集不变。

- 训练样本权重
 Bagging：每个训练样本的权重相等，即1/N。
Boosting：根据学习器的错误率不断调整样例的权值，错误率越大，权值越大。

- 预测函数的权重：
Bagging：K组学习器的权重相等，即1/K。
Boosting：学习器性能好的分配较大的权重，学习器性能差的分配较小的权重。

- 并行计算
 Bagging：K组学习器模型可以并行生成。
Boosting：K组学习器只能顺序生成，因为后一个模型的样本权值需要前一个学习器模型的结果。

#### Bagging和Boosting的方差和偏差问题
- Bagging
bagging对样本进行有放回的重采样，学习结果是各个学习模型的平均值。由于重采样的样本集具有相似性以及使用相同的学习器模型，因此，各学习模型的结果相近，即模型有近似相等的偏差和方差。
假设Xi为 i 组训练样本集，各组训练样本集是相互独立的，不同的训练样本集代表不同的模型，由概率论可知
对于bagging来说我们设总模型的预测结果为$F=\sum \limits_{i=1}^{n}w_if_i$：
则期望计算为：
$$E[F]=E[\sum \limits_{i=1}^{n}w_if_i]=
\sum \limits_{i=1}^{n}w_iE(f_i)$$
由于bagging每个基模型的权重一样，上式就等于
$m\frac{1}{n}E(f_i)=E(f_i)$
又由于基模型期望近似相等，所以bagging的总期望等于基模型的期望



$$Var(F)=Var(\sum \limits_{i=1}^{n}w_if_i)\\
=\sum \limits_{i=1}^{n}Var(w_if_i)+\sum \limits_{i!=j}^{n}cov(w_if_i,wjfj)\\
=n*w^2*\sigma^2(1-\rho)+n(n-1)\rho w^2\sigma^2\\
=nw^2\sigma^2(1-\rho)+n^2w^2\sigma^2\rho$$
由于每个基模型的权重一样$w=\frac{1}{n}$
整理上式，等于
$$\frac{\sigma^2(1-\rho)}{n}+\sigma^2\rho\\
=\sigma^2(\frac{1+(m-1)\rho}{m})$$
观察上式，当基模型强相关的时候$\rho=1$，基模型方差等于模型总体方差
bagging采用样本采样来降低模型相关性$\rho$，增加基模型数量，可以减少方差

- boosting
 Boosting 框架采用基于贪心策略的前向加法，整体模型的期望由基模型的期望累加而成，随着基模型数量增多，整体模型的期望值增加，准确度提高。
$$\sigma^2(\frac{1+(m-1)\rho}{m})$$
 对于方差来说，基模型强相关，所以$\rho=1$，所以整体方差等于基模型的方差

#### 关系
1） Bagging + 决策树 = 随机森林

2）AdaBoost + 决策树 = 提升树

3）Gradient Boosting + 决策树 = GBDT